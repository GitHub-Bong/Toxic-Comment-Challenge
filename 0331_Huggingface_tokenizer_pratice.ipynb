{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0331-Huggingface_tokenizer-pratice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPucC9yGS3J9YA1DXXJx5lE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitHub-Bong/Toxic-Comment-Challenge/blob/master/0331_Huggingface_tokenizer_pratice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUWpEtmFNy_X"
      },
      "source": [
        "**byte level bpe tokenizer**     \n",
        "<br/>\n",
        "__char bpe tokenizer__      \n",
        "<br/>\n",
        "__sentencepiece bpe tokenizer__    \n",
        "<br/>\n",
        "__bert wordpiece tokenizer__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh6qQ3BjNezr",
        "outputId": "930706ce-2fa2-4a69-830e-cad9222231df"
      },
      "source": [
        "pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A15ucxzOEGu"
      },
      "source": [
        "from tokenizers import SentencePieceBPETokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXQ2Cep1OPw0",
        "outputId": "a404c454-1f16-418a-9743-2bf9213c4b6e"
      },
      "source": [
        "tokenizer = SentencePieceBPETokenizer()\n",
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=0, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBBcfwBROVv_"
      },
      "source": [
        "There are various parameters    \n",
        "<br/>\n",
        "Just set vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJt5_0PiOlzs"
      },
      "source": [
        "tokenizer.train([],vocab_size=30000)\n",
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUjbgdLRP1g1"
      },
      "source": [
        "save "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxZiIdlOP3V3"
      },
      "source": [
        "tokenizer.save_model('/content/drive/Shareddrives/SOGANG Parrot/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In8hdYYzQWzH"
      },
      "source": [
        "vocab, merges = tokenizer.save_model('/content/drive/Shareddrives/SOGANG Parrot/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA9vVAeaQga8"
      },
      "source": [
        "load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jih1KMeLQhoU"
      },
      "source": [
        "tokenizer = SentencePieceBPETokenizer(\n",
        "    vocab_file = vocab, \n",
        "    merges_file = merges\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5OGHmUbPa29"
      },
      "source": [
        "------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hxDvxNSPmfE"
      },
      "source": [
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5tyjDndPb07",
        "outputId": "3dd6ddf4-85f9-4855-e23c-575156b7f973"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBIKxh9SPd5-"
      },
      "source": [
        "import sys, os, re, csv, codecs, numpy as np, pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9uOh5w2PjHc"
      },
      "source": [
        "train = pd.read_csv('/content/drive/Shareddrives/SOGANG Parrot/train.csv/train.csv')\n",
        "test = pd.read_csv('/content/drive/Shareddrives/SOGANG Parrot/test.csv/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiFw5_CYPjuE"
      },
      "source": [
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "y = train[list_classes].values # y.shape (159571, 6)\n",
        "list_sentences_train = train[\"comment_text\"] # (159571,)\n",
        "list_sentences_test = test[\"comment_text\"] # (153164,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCOHq30bd2Nb"
      },
      "source": [
        "' '.join(list_sentences_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "MzzF3HSRPjwj",
        "outputId": "a7945bc1-7d21-4696-d745-8094bac97114"
      },
      "source": [
        "tokenizer.train([list_sentences_test],vocab_size=30000)\n",
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e0a7f07b64ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_sentences_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tokenizers/implementations/sentencepiece_bpe.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, files, vocab_size, min_frequency, special_tokens, limit_alphabet, initial_alphabet, show_progress)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     def train_from_iterator(\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't convert 0         Yo bitch Ja Rule is more succesful then you'll...\n1         == From RfC == \\n\\n The title is fine as it is...\n2         \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3         :If you have a look back at the source, the in...\n4                 I don't anonymously edit articles at all.\n                                ...                        \n153159    . \\n i totally agree, this stuff is nothing bu...\n153160    == Throw from out field to home plate. == \\n\\n...\n153161    \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n153162    \" \\n\\n == \"\"One of the founding nations of the...\n153163    \" \\n :::Stop already. Your bullshit is not wel...\nName: comment_text, Length: 153164, dtype: object to PyString"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k8nu7hkPj42",
        "outputId": "1012edbe-2404-4934-d848-4eb5d0deaad8"
      },
      "source": [
        "list_sentences_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(159571,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "oQO02EqjT_Oi",
        "outputId": "90271e59-f469-4a02-98c4-d4cef94ebc9e"
      },
      "source": [
        "list_sentences_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuUQdscAUBAK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}